import requests
from bs4 import BeautifulSoup
from urllib.request import urlretrieve
import time
import random

start = 1
end = 167

base_link = "https://github.com/advisories?page=1&query=type%3Areviewed+ecosystem%3Amaven+sort%3Apublished-desc"
CVE_FILE = open("CVE_LIST.txt","w")
for i in range(end):
    link = "https://github.com/advisories?page="+str(i+start) +"&query=type%3Areviewed+ecosystem%3Amaven+sort%3Apublished-desc"
    page = requests.get(link).content
    soup = BeautifulSoup(page, 'lxml')

    
    List = soup.find_all("span", {"class": lambda c: c and 'text-bold' in c})

    for entry in List:
        if(entry.get_text().find("GHSA")!= -1):
            print("Skipped GHSA")
            continue
        print(entry.get_text())
        CVE_FILE.write(entry.get_text().strip() + "\n")




    pass