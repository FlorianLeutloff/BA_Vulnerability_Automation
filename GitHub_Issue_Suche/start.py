import requests
from bs4 import BeautifulSoup
from urllib.request import urlretrieve
import time
import random

baseurl = "https://www.google.com/search?q="

Additional_Search_Term = "+" + "github"


CVE_FILE = open("CVE_LIST.txt","r")

CVE_list = CVE_FILE.readlines()

CVE_FILE.close()

CVE_FILE = open("CVE_SOURCES.txt","w")



for cve in CVE_list:
    print(cve)
    CVE_FILE.write(cve + "\n")
    targeturl = baseurl + cve.strip() + Additional_Search_Term
    time.sleep(1)
    page = requests.get(targeturl,cookies={'__Secure-ENID':'', #eigene cookies benötigt, für Google Suche
                                           'AEC':'',
                                           'CONSENT':'',
                                           'DV':'',
                                           'SOCS':''}).content
    soup = BeautifulSoup(page, 'lxml')
    print(soup.body)
    try:
        for item in soup.body.find("div",{"id": lambda c: c and 'main' in c}).find_all("div", {"class": lambda c: c and 'Gx5Zad fP1Qef xpd EtOod pkphOe' in c}):
            print("Test")
            print(item)
            link = item.find("a").get("href")
            link = link.replace("/url?q=","")
            link = link.split("&sa=")[0]
            print(link)
            CVE_FILE.write(link + "\n")
            pass
    except:
        print("Error in retrieving Link List")
        pass
    CVE_FILE.write("##############\n")
    pass
