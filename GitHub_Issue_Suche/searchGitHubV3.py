import requests
from bs4 import BeautifulSoup
from urllib.request import urlretrieve
import time
import random

baseurl = "https://api.github.com/search/issues?q=" #+ CVE

#Additional_Search_Term = "&s=comments&o=desc" #search for msot commented
#Category_List = ["&type=issues","&type=discussions"]

CVE_FILE = open("CVE_LIST.txt","r")

CVE_list = CVE_FILE.readlines()

CVE_FILE.close()

CVE_FILE = open("CVE_SOURCES.txt","w")

#CVE_list = ["CVE-2016-1000027"]

for cve in CVE_list:

    #only scrape the first 1000
    counter = 0
    if(counter == 1000):
        break
    print(cve)
    CVE_FILE.write(cve + "\n")
    cve = cve.replace("\n","")
    targeturl = baseurl +"\"" + cve.strip() +"\"" + "+is:issue" + "&sort=comments&order=desc&per_page=100"
    print(targeturl)
    time.sleep(2)
    response = requests.get(targeturl, headers={"Accept":"application/vnd.github+json","Authorization":"[Ben√∂tigt euren GitHub API Token]","X-GitHub-Api-Version":"2022-11-28"})
    json_file = response.json()

    for part in json_file.get('items'):
        if(part.get('user').get('login').find("[bot]")!=-1):
            CVE_FILE.write(part.get('html_url') + "[BOT]" + "\n")
            continue
        CVE_FILE.write(part.get('html_url') + "\n")
        pass
    pass
    CVE_FILE.write("##############\n")

    #only scrape the first 1000
    counter = counter + 1
    pass
